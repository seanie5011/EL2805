----------
model_000000.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 100
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 10000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 1.0
final average reward: -62
----------
model_000001.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 100
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 10000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 1.0
final average reward: -78
----------
model_000002.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 100
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 1.0
final average reward: -60
----------
model_000003.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 100
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 1.0
final average reward: -63
----------
model_000004.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 100
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -56
----------
model_000005.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 100
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -45
----------
model_000006.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 100
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.0001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -83
----------
model_000007.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 100
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.0001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -75
----------
model_000008.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 35
----------
model_000009.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 23
----------
model_000010.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 64
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 8
----------
model_000011.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 64
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -2.5
----------
model_000012.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 49
----------
model_000013.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 19
----------
model_000014.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 500
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -46
----------
model_000015.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 220
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 30
----------
model_000016.pth
Architecture: {input_size, 32, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 300
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -157
----------
model_000017.pth
Architecture: {input_size, 16, 16, output_size} with ReLU activation on input and hidden layer
N_episodes: 300
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -23
----------
model_000018.pth
Architecture: {input_size, 16, 16, output_size} with ReLU activation on input and hidden layer
N_episodes: 300
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.0001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -14
----------
model_000019.pth
Architecture: {input_size, 16, 16, output_size} with ReLU activation on input and hidden layer
N_episodes: 500
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.0001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -73
----------
model_000020.pth
Architecture: {input_size, 32, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 500
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.0001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -74
----------
model_000021.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 300
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -90
----------
model_000022.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 300
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -53
----------
model_000023.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 16
----------
model_000024.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 29
----------
model_000025.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.99
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 13
----------
model_000026.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 500
discount_factor: 0.99
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -146
----------
model_000028.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 220
discount_factor: 0.99
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 72
----------
model_000029.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -99
----------
model_000030.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 1.0
final average reward: -102
----------
model_000031.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -70
----------
model_000032.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 300
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -72
----------
model_000033.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -31
----------
model_000034.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 64
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -5
----------
model_000035.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -48
----------
model_000036.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.002
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -54
----------
model_000037.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 250
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.002
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -93
----------
model_000038.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 250
discount_factor: 0.95
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -64
----------
model_000039.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.99
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -103
----------
model_000040.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.92
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -114
----------
model_000041.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 250
discount_factor: 0.9
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -62
----------
model_000042.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 300
discount_factor: 0.9
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -32
----------
model_000043.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 350
discount_factor: 0.9
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -66
----------
model_000044.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 500
discount_factor: 0.9
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -52
----------
model_000045.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 500
discount_factor: 0.85
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 400
clip_max_norm: 0.5
final average reward: -60
----------
model_000046.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.85
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 0.5
final average reward: -77
----------
model_000047.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.9
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 1.5
final average reward: -41
----------
model_000048.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 220
discount_factor: 0.9
n_ep_running_average: 50
batch_size: 64
buffer_size: 25000
learning_rate: 0.0012
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 200
clip_max_norm: 1.5
final average reward: -65
----------
model_000049.pth
Architecture: {input_size, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.92
n_ep_running_average: 50
batch_size: 64
buffer_size: 30000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.1
epsilon_episodes: 200
clip_max_norm: 1.5
final average reward: -79
----------
model_000050.pth
Architecture: {input_size, 64, 64, output_size} with ReLU activation on input and hidden layer
N_episodes: 200
discount_factor: 0.92
n_ep_running_average: 50
batch_size: 64
buffer_size: 30000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.1
epsilon_episodes: 200
clip_max_norm: 1.5
final average reward: -73
----------
[Back to improper code]
model_000051.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 220
discount_factor: 1.0
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -644
----------
[Back to improper code]
model_000052.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 220
discount_factor: 0.01
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -91
----------
[Back to improper code]
model_000053.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 150
discount_factor: 0.99
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: -38
----------
[Back to improper code]
model_000054.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 300
discount_factor: 0.99
n_ep_running_average: 50
batch_size: 32
buffer_size: 20000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 72
----------
[Back to improper code]
model_000055.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 220
discount_factor: 0.99
n_ep_running_average: 50
batch_size: 32
buffer_size: 30000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 0
----------
[Back to improper code]
model_000056.pth
Architecture: {input_size, 32, output_size} with ReLU activation on input and hidden layer
N_episodes: 220
discount_factor: 0.99
n_ep_running_average: 50
batch_size: 32
buffer_size: 10000
learning_rate: 0.001
epsilon_max: 0.95
epsilon_min: 0.05
epsilon_episodes: 500
clip_max_norm: 0.5
final average reward: 27